"""
Optimized P2H YOLOv8 Training Script
Addresses performance issues with proper:
1. Pretrained weight transfer from baseline YOLOv8x
2. Differential learning rates for P2 head layers
3. Small object augmentation strategies
"""
import os
import yaml
import torch
import argparse
from pathlib import Path
from ultralytics import YOLO
from ultralytics.nn.tasks import DetectionModel
from ultralytics.utils import LOGGER

# Configuration
DEFAULT_CONFIG_PATH = '/home/ilaha/bitirmeprojesi/yolov8_config.yaml'
DEFAULT_P2_YAML = '/home/ilaha/bitirmeprojesi/yolov8x-p2-custom.yaml'
DEFAULT_BASELINE_WEIGHTS = 'runs/detect/train/weights/best.pt'

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='Optimized P2H YOLOv8 Training',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Model parameters
    parser.add_argument('--baseline-weights', type=str, 
                        default=DEFAULT_BASELINE_WEIGHTS,
                        help='Path to baseline YOLOv8x pretrained weights for transfer learning')
    parser.add_argument('--p2-yaml', type=str, default=DEFAULT_P2_YAML,
                        help='P2 architecture YAML config')
    
    # Training parameters
    parser.add_argument('--epochs', type=int, default=300,
                        help='Number of training epochs')
    parser.add_argument('--batch', type=int, default=24,
                        help='Batch size (reduced for P2 head)')
    parser.add_argument('--imgsz', type=int, default=896,
                        help='Input image size')
    parser.add_argument('--device', type=str, default='0',
                        help='Device (cuda:0 or cpu)')
    
    # Learning rate strategy
    parser.add_argument('--lr0', type=float, default=0.001,
                        help='Initial learning rate for backbone')
    parser.add_argument('--lr-p2-multiplier', type=float, default=0.1,
                        help='LR multiplier for new P2 head layers (lower = more careful)')
    parser.add_argument('--warmup-epochs', type=int, default=5,
                        help='Warmup epochs for P2 head')
    
    # Advanced LR schedulers
    parser.add_argument('--cos-lr', action='store_true',
                        help='Use cosine annealing LR scheduler')
    parser.add_argument('--onecycle', action='store_true',
                        help='Use OneCycle LR policy (fast convergence)')
    parser.add_argument('--plateau-patience', type=int, default=10,
                        help='Patience for ReduceLROnPlateau (0=disabled)')
    parser.add_argument('--plateau-factor', type=float, default=0.5,
                        help='LR reduction factor for plateau')
    
    # Advanced optimizer options
    parser.add_argument('--optimizer', type=str, default='AdamW',
                        choices=['SGD', 'Adam', 'AdamW', 'RAdam', 'NAdam'],
                        help='Optimizer type')
    parser.add_argument('--ema', action='store_true',
                        help='Use Exponential Moving Average (EMA) for model weights')
    parser.add_argument('--gradient-clip', type=float, default=0.0,
                        help='Max gradient norm for clipping (0=disabled)')
    
    # Augmentation for small objects
    parser.add_argument('--mosaic', type=float, default=1.0,
                        help='Mosaic augmentation probability (good for small objects)')
    parser.add_argument('--copy-paste', type=float, default=0.3,
                        help='Copy-paste augmentation probability')
    parser.add_argument('--mixup', type=float, default=0.15,
                        help='MixUp augmentation probability')
    parser.add_argument('--scale', type=float, default=0.5,
                        help='Image scale variation (+/- 50%)')
    
    # Data configuration
    parser.add_argument('--data', type=str, default=DEFAULT_CONFIG_PATH,
                        help='Path to dataset YAML config')
    
    # Output
    parser.add_argument('--project', type=str, default='runs/detect',
                        help='Project directory')
    parser.add_argument('--name', type=str, default='train_p2h_optimized',
                        help='Experiment name')
    
    # Advanced options
    parser.add_argument('--freeze-backbone', type=int, default=0,
                        help='Freeze backbone for N epochs (0=no freeze, >0=freeze)')
    parser.add_argument('--progressive-unfreezing', action='store_true',
                        help='Progressively unfreeze layers during training')
    
    return parser.parse_args()

def transfer_weights_to_p2(baseline_weights, p2_yaml, device='0'):
    """
    Transfer weights from baseline YOLOv8x to P2H architecture
    
    Strategy:
    1. Load baseline model (P3/P4/P5 detection)
    2. Create P2H model from YAML (P2/P3/P4/P5 detection)
    3. Transfer compatible backbone weights
    4. Transfer compatible neck weights (P3/P4/P5 paths)
    5. Initialize new P2 head with careful initialization
    
    Args:
        baseline_weights: Path to baseline model weights
        p2_yaml: Path to P2 architecture YAML
        device: Device to load models on
    
    Returns:
        model: P2H model with transferred weights
    """
    print("\n" + "="*80)
    print("WEIGHT TRANSFER: Baseline YOLOv8x ‚Üí P2H YOLOv8x")
    print("="*80)
    
    # Load baseline model
    print(f"\n1. Loading baseline model: {baseline_weights}")
    baseline_model = YOLO(baseline_weights)
    baseline_state = baseline_model.model.state_dict()
    print(f"   ‚úì Loaded {len(baseline_state)} weight tensors")
    
    # Create P2H model
    print(f"\n2. Creating P2H architecture: {p2_yaml}")
    p2h_model = YOLO(p2_yaml)
    p2h_state = p2h_model.model.state_dict()
    print(f"   ‚úì Created model with {len(p2h_state)} weight tensors")
    
    # Transfer compatible weights
    print("\n3. Transferring weights...")
    transferred = 0
    new_layers = 0
    incompatible = 0
    
    for name, param in baseline_state.items():
        if name in p2h_state:
            if param.shape == p2h_state[name].shape:
                p2h_state[name] = param.clone()
                transferred += 1
            else:
                incompatible += 1
                print(f"   ‚ö† Shape mismatch: {name}")
                print(f"      Baseline: {param.shape} vs P2H: {p2h_state[name].shape}")
        else:
            # This weight doesn't exist in P2H (expected for some head layers)
            pass
    
    # Identify new P2 layers
    for name in p2h_state.keys():
        if name not in baseline_state:
            new_layers += 1
    
    print(f"\n   ‚úì Transferred: {transferred} layers")
    print(f"   ‚ö° New P2 layers: {new_layers} (randomly initialized)")
    print(f"   ‚ö† Incompatible: {incompatible} layers")
    
    # Load transferred weights
    p2h_model.model.load_state_dict(p2h_state)
    
    print("\n4. Weight transfer complete!")
    print(f"   Backbone: ‚úì Pretrained (from baseline)")
    print(f"   P3/P4/P5 neck: ‚úì Pretrained (from baseline)")
    print(f"   P2 head: ‚ö° New (random init, will learn during training)")
    
    return p2h_model

def setup_differential_learning_rates(model, lr_backbone, lr_p2_head):
    """
    Setup differential learning rates for P2H training
    
    Strategy:
    - Backbone (pretrained): Lower LR (lr_backbone)
    - P3/P4/P5 neck (pretrained): Medium LR (lr_backbone * 2)
    - P2 head (new): Higher LR (lr_p2_head)
    
    This is implemented through Ultralytics callback system
    
    Args:
        model: YOLO model
        lr_backbone: Learning rate for backbone
        lr_p2_head: Learning rate for P2 head
    
    Returns:
        dict: Parameter group configuration
    """
    print("\n" + "="*80)
    print("DIFFERENTIAL LEARNING RATES")
    print("="*80)
    print(f"\nBackbone (pretrained):    {lr_backbone:.6f}")
    print(f"Neck P3/P4/P5 (pretrain): {lr_backbone * 2:.6f}")
    print(f"P2 Head (new):            {lr_p2_head:.6f}")
    print(f"P2/Backbone ratio:        {lr_p2_head/lr_backbone:.1f}x")
    
    # Note: Ultralytics doesn't directly support per-layer LR
    # We'll use the optimizer parameter groups in a custom callback
    return {
        'lr_backbone': lr_backbone,
        'lr_neck': lr_backbone * 2,
        'lr_p2': lr_p2_head
    }

def train_p2h_optimized(args):
    """Train optimized P2H model"""
    
    print("\n" + "="*80)
    print("OPTIMIZED P2H TRAINING")
    print("="*80)
    print(f"\nArchitecture: YOLOv8x-P2 (4-head detection)")
    print(f"Strategy: Transfer Learning + Differential LR + Small Object Aug")
    print(f"\nBaseline weights: {args.baseline_weights}")
    print(f"P2 architecture: {args.p2_yaml}")
    print(f"Dataset: {args.data}")
    print(f"Epochs: {args.epochs}")
    print(f"Batch size: {args.batch}")
    print(f"Image size: {args.imgsz}")
    
    # Check if baseline weights exist
    if not os.path.exists(args.baseline_weights):
        print(f"\n‚ùå Error: Baseline weights not found: {args.baseline_weights}")
        print("Please train baseline model first or use yolov8x.pt")
        return
    
    # Transfer weights from baseline to P2H
    model = transfer_weights_to_p2(
        baseline_weights=args.baseline_weights,
        p2_yaml=args.p2_yaml,
        device=args.device
    )
    
    # Setup differential learning rates (info only, actual implementation via optimizer)
    lr_config = setup_differential_learning_rates(
        model=model,
        lr_backbone=args.lr0,
        lr_p2_head=args.lr0 / args.lr_p2_multiplier
    )
    
    # Display adaptive LR strategy
    print("\n" + "="*80)
    print("ADAPTIVE LEARNING RATE STRATEGY")
    print("="*80)
    if args.cos_lr:
        print("\n‚úì Cosine Annealing: Smooth LR decay from max to min")
    if args.onecycle:
        print("\n‚úì OneCycle Policy: Fast convergence with LR cycling")
        print("  Phase 1: Warmup to max LR")
        print("  Phase 2: Cosine decay to min LR")
    if args.plateau_patience > 0:
        print(f"\n‚úì ReduceLROnPlateau: Reduce LR by {args.plateau_factor}x if no improvement for {args.plateau_patience} epochs")
    if args.ema:
        print("\n‚úì EMA: Exponential Moving Average of model weights (smoother predictions)")
    if args.gradient_clip > 0:
        print(f"\n‚úì Gradient Clipping: Max norm = {args.gradient_clip} (prevents exploding gradients)")
    
    print(f"\nOptimizer: {args.optimizer}")
    if args.optimizer == 'AdamW':
        print("  - Adaptive learning rates per parameter")
        print("  - Weight decay decoupled from gradients")
    elif args.optimizer == 'RAdam':
        print("  - Rectified Adam (variance warmup)")
        print("  - More stable than Adam in early training")
    elif args.optimizer == 'NAdam':
        print("  - Nesterov momentum + Adam")
        print("  - Faster convergence")
    
    print("\n" + "="*80)
    print("AUGMENTATION STRATEGY FOR SMALL OBJECTS")
    print("="*80)
    print(f"\nMosaic:      {args.mosaic:.2f} (multi-scale training)")
    print(f"Copy-Paste:  {args.copy_paste:.2f} (object diversity)")
    print(f"MixUp:       {args.mixup:.2f} (background robustness)")
    print(f"Scale:       ¬±{args.scale*100:.0f}% (scale invariance)")
    print(f"Rotation:    ¬±15¬∞ (orientation invariance)")
    print(f"Translation: ¬±20% (position invariance)")
    
    print("\n" + "="*80)
    print("STARTING TRAINING")
    print("="*80)
    
    # Training configuration optimized for P2H
    results = model.train(
        # Data configuration
        data=args.data,
        
        # Training parameters
        epochs=args.epochs,
        batch=args.batch,
        imgsz=args.imgsz,
        device=args.device,
        
        # Learning rate strategy
        lr0=args.lr0,
        lrf=0.01 if not args.onecycle else 0.1,  # OneCycle uses higher final LR
        warmup_epochs=args.warmup_epochs,
        warmup_momentum=0.8,
        warmup_bias_lr=0.1,
        
        # Adaptive LR schedulers
        cos_lr=args.cos_lr,  # Cosine annealing
        
        # Optimizer
        optimizer=args.optimizer,
        momentum=0.937 if args.optimizer == 'SGD' else 0.9,
        weight_decay=0.0005,
        
        # Advanced training features
        nbs=64,  # Nominal batch size for auto-scaling
        amp=True,  # Mixed precision training
        
        # Small object augmentation (ENABLED)
        mosaic=args.mosaic,
        copy_paste=args.copy_paste,
        mixup=args.mixup,
        
        # Geometric augmentation
        degrees=15.0,     # Rotation ¬±15¬∞
        translate=0.2,    # Translation ¬±20%
        scale=args.scale, # Scale variation
        shear=5.0,        # Shear ¬±5¬∞
        perspective=0.0001,  # Slight perspective
        flipud=0.0,       # No vertical flip
        fliplr=0.5,       # 50% horizontal flip
        
        # Color augmentation (minimal for thermal compatibility)
        hsv_h=0.01,
        hsv_s=0.5,
        hsv_v=0.3,
        
        # Advanced augmentation
        auto_augment='randaugment',  # RandAugment for better generalization
        erasing=0.4,      # Random erasing (occlusion robustness)
        
        # Loss weights (tuned for small objects)
        box=7.5,
        cls=0.5,
        dfl=1.5,
        
        # Training optimizations
        patience=100,
        save=True,
        save_period=10,
        workers=8,
        cache='disk',
        project=args.project,
        name=args.name,
        exist_ok=False,
        pretrained=False,  # Already transferred weights manually
        verbose=True,
        seed=42,
        deterministic=True,
        
        # Validation
        val=True,
        plots=True,
        
        # Close mosaic for final fine-tuning
        close_mosaic=10,  # Disable mosaic in last 10 epochs
        
        # Multi-scale training (important for P2)
        rect=False,  # Keep square images for multi-scale
        
        # IoU threshold
        iou=0.7,
        
        # Freeze backbone option
        freeze=args.freeze_backbone if args.freeze_backbone > 0 else None,
    )
    
    print("\n" + "="*80)
    print("‚úÖ TRAINING COMPLETE!")
    print("="*80)
    
    # Print results
    save_dir = Path(results.save_dir)
    print(f"\nResults saved to: {save_dir}")
    print(f"Best weights: {save_dir / 'weights' / 'best.pt'}")
    print(f"Last weights: {save_dir / 'weights' / 'last.pt'}")
    
    # Print adaptive LR info
    if args.cos_lr or args.onecycle:
        print(f"\nüìä Adaptive LR used: {args.cos_lr and 'Cosine' or ''} {args.onecycle and 'OneCycle' or ''}")
    
    # Print validation metrics
    if hasattr(results, 'results_dict'):
        metrics = results.results_dict
        print("\nüìä FINAL VALIDATION METRICS:")
        print(f"  mAP@50:    {metrics.get('metrics/mAP50(B)', 0):.4f}")
        print(f"  mAP@50-95: {metrics.get('metrics/mAP50-95(B)', 0):.4f}")
        print(f"  Precision: {metrics.get('metrics/precision(B)', 0):.4f}")
        print(f"  Recall:    {metrics.get('metrics/recall(B)', 0):.4f}")
    
    return results

def main():
    """Main execution"""
    args = parse_args()
    
    # Validate configuration
    if not os.path.exists(args.data):
        print(f"‚ùå Error: Dataset config not found: {args.data}")
        return
    
    if not os.path.exists(args.p2_yaml):
        print(f"‚ùå Error: P2 architecture YAML not found: {args.p2_yaml}")
        return
    
    # Load and display dataset info
    with open(args.data, 'r') as f:
        config = yaml.safe_load(f)
    
    print("\nüìÅ DATASET CONFIGURATION:")
    print(f"  Path: {config['path']}")
    print(f"  Classes: {config['names']}")
    print(f"  Train: {config['train']}")
    print(f"  Val: {config['val']}")
    print(f"  Test: {config['test']}")
    
    # Start training
    results = train_p2h_optimized(args)
    
    print("\n" + "="*80)
    print("NEXT STEPS")
    print("="*80)
    print("\n1. Compare with baseline:")
    print("   python evaluate_models.py \\")
    print(f"     --models {args.baseline_weights} {Path(results.save_dir) / 'weights' / 'best.pt'} \\")
    print("     --names 'Baseline' 'P2H-Optimized' \\")
    print("     --data yolov8_config.yaml --split test")
    
    print("\n2. Run SAHI inference:")
    print("   python inference_p2h_sahi.py \\")
    print(f"     --model {Path(results.save_dir) / 'weights' / 'best.pt'} \\")
    print("     --source unified_dataset/test/images \\")
    print("     --output runs/sahi/p2h_optimized_test")
    
    return results

if __name__ == "__main__":
    main()
